# Databricks notebook source
# ============================================================
# 1Ô∏è‚É£ STORAGE CONFIGURATION
# ============================================================
storage_account = "storage_Account_name"
storage_key = "storage_key"

spark.conf.set(
    f"fs.azure.account.key.{storage_account}.blob.core.windows.net",
    storage_key
)
print("‚úî Storage Connected")


# ============================================================
# 2Ô∏è‚É£ LOAD BRONZE JSON STREAM FILES
# ============================================================
from pyspark.sql.functions import explode, col

bronze_path = f"wasbs://bronze@{storage_account}.blob.core.windows.net/api_streams/"

raw_df = spark.read.json(bronze_path, multiLine=True)
print("üì• Bronze Loaded")
raw_df.printSchema()


# ============================================================
# 3Ô∏è‚É£ CLEAN COLUMN NAME FUNCTION
# ============================================================
def clean_cols(df):
    for c in df.columns:
        new = (
            c.strip()
            .replace(" ", "_")
            .replace("(", "")
            .replace(")", "")
            .replace(",", "")
            .replace("-", "_")
            .replace(".", "_")
        )
        df = df.withColumnRenamed(c, new)
    return df


# ============================================================
# 4Ô∏è‚É£ SILVER ‚Äî ENVIRONMENT TABLE
# ============================================================
df_env = (
    raw_df
        .select(explode("env_records").alias("env"))
        .select("env.*")
)

df_env = clean_cols(df_env).dropDuplicates()

df_env.write.mode("overwrite").parquet(
    f"wasbs://silver@{storage_account}.blob.core.windows.net/env/"
)

print("üåø ENV ‚Üí Silver Completed")


# ============================================================
# 5Ô∏è‚É£ SILVER ‚Äî HEALTH TABLE (FINAL FIX)
# ============================================================

df_health = (
    raw_df
        .select(explode("health_stream").alias("h"))
        .select(
            col("h.Cattle_ID").alias("Cattle_ID"),   # STRING ID like CATTLE_1234
            col("h.HealthScore").cast("int"),
            col("h.LiveFeedIntake").cast("double"),
            col("h.LiveMilkYield").cast("double"),
            col("h.LiveTemperature").cast("double"),
            col("h.Region"),
            col("h.Severity")
        )
)

df_health = clean_cols(df_health).dropDuplicates()

df_health.write.mode("overwrite").parquet(
    f"wasbs://silver@{storage_account}.blob.core.windows.net/health/"
)

print("‚ù§Ô∏è HEALTH ‚Üí Silver Completed (Cattle_ID as STRING)")


# ============================================================
# 6Ô∏è‚É£ SILVER ‚Äî ALERTS TABLE
# ============================================================
df_alerts = (
    raw_df
        .select(explode("alerts").alias("a"))
        .select("a.*")
)

df_alerts = clean_cols(df_alerts).dropDuplicates()

df_alerts.write.mode("overwrite").parquet(
    f"wasbs://silver@{storage_account}.blob.core.windows.net/alerts/"
)

print("üö® ALERTS ‚Üí Silver Completed")


# ============================================================
# 7Ô∏è‚É£ VERIFY SILVER OUTPUT
# ============================================================
print("üìÇ SILVER FOLDERS:")
display(dbutils.fs.ls(f"wasbs://silver@{storage_account}.blob.core.windows.net/"))


# COMMAND ----------

# ============================================================
# 0Ô∏è‚É£ HELPER: CLEAN COLUMN NAMES
# ============================================================
def clean_cols(df):
    for c in df.columns:
        new = (
            c.strip()
             .replace(" ", "_")
             .replace("(", "")
             .replace(")", "")
             .replace(",", "")
             .replace("-", "_")
             .replace(".", "_")
        )
        df = df.withColumnRenamed(c, new)
    return df


# ============================================================
# 1Ô∏è‚É£ PREGNANCY ‚Üí SILVER
# ============================================================
pregnancy_path = (
    "wasbs://bronze@herdtrackstorage1.blob.core.windows.net/static/pregnancy_record.csv"
)

df_preg = (
    spark.read.option("header", True)
        .option("inferSchema", True)
        .csv(pregnancy_path)
)

print("üì• Pregnancy CSV Loaded")
df_preg.printSchema()

df_preg = clean_cols(df_preg)

df_preg.write.mode("overwrite").parquet(
    "wasbs://silver@herdtrackstorage1.blob.core.windows.net/pregnancy/"
)

print("ü§± PREGNANCY ‚Üí Silver Completed")


# ============================================================
# 2Ô∏è‚É£ FEED TYPE ‚Üí SILVER  (optional but good for analytics)
# ============================================================
feed_path = (
    "wasbs://bronze@herdtrackstorage1.blob.core.windows.net/static/feed_type.csv"
)

df_feed = (
    spark.read.option("header", True)
        .option("inferSchema", True)
        .csv(feed_path)
)

print("üì• Feed CSV Loaded")
df_feed.printSchema()

df_feed = clean_cols(df_feed)

df_feed.write.mode("overwrite").parquet(
    "wasbs://silver@herdtrackstorage1.blob.core.windows.net/feed_type/"
)

print("üåæ FEED TYPE ‚Üí Silver Completed")


# ============================================================
# 3Ô∏è‚É£ SENSOR METADATA ‚Üí SILVER  (optional)
# ============================================================
sensor_path = (
    "wasbs://bronze@herdtrackstorage1.blob.core.windows.net/static/sensor_metadata.csv"
)

df_sensor = (
    spark.read.option("header", True)
        .option("inferSchema", True)
        .csv(sensor_path)
)

print("üì• Sensor CSV Loaded")
df_sensor.printSchema()

df_sensor = clean_cols(df_sensor)

df_sensor.write.mode("overwrite").parquet(
    "wasbs://silver@herdtrackstorage1.blob.core.windows.net/sensor_metadata/"
)

print("üìü SENSOR METADATA ‚Üí Silver Completed")


# ============================================================
# 4Ô∏è‚É£ CHECK ALL SILVER STATIC TABLES
# ============================================================
display(dbutils.fs.ls("wasbs://silver@herdtrackstorage1.blob.core.windows.net/"))


# ============================================================
# LOAD SENSOR METADATA STATIC CSV FROM BRONZE ‚Üí SILVER
# ============================================================

sensor_path = f"wasbs://bronze@{storage_account}.blob.core.windows.net/static/sensor_metadata.csv"

df_sensor = spark.read.csv(sensor_path, header=True, inferSchema=True)

df_sensor = df_sensor.dropDuplicates()

df_sensor.write.mode("overwrite").parquet(
    f"wasbs://silver@{storage_account}.blob.core.windows.net/sensor/"
)

print("üîß SENSOR ‚Üí Silver Completed")




# ============================================================
# SILVER SENSOR (from Bronze/static/sensor_metadata.csv)
# ============================================================

sensor_path = f"wasbs://bronze@{storage_account}.blob.core.windows.net/static/sensor_metadata.csv"

df_sensor = spark.read.csv(sensor_path, header=True, inferSchema=True)

df_sensor = df_sensor.dropDuplicates()

df_sensor.write.mode("overwrite").parquet(
    f"wasbs://silver@{storage_account}.blob.core.windows.net/sensor/"
)

print("üîß SENSOR ‚Üí Silver Completed")




display(dbutils.fs.ls(f"wasbs://silver@{storage_account}.blob.core.windows.net/sensor/"))


storage_account = "storage_account_name"
storage_key = "storage_key"

spark.conf.set(
    f"fs.azure.account.key.{storage_account}.blob.core.windows.net",
    storage_key
)
print("‚úî Storage Connected")